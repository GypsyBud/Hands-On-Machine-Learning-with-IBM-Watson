{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Buiding Your First Machine Learning Model\n",
    "\n",
    "In this Tutorial you will learn how to build a Convolutional Neural Network (CNN) to identify handwriting. It is an introductory tutorial that will teach you how to create the different layers in a CNN using the tensorflow library. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is a Neural Network\n",
    "\n",
    "A neural network (NN) is a machine learning model that is used to classify and predict a certain variable using a set of features in the data. Typically, it consists of an input layer that takes in the features, a number of hidden layers (more layer -> deep NN), and an oputput layer that returns the prediction. The layers in the NN are are composed of Neurons (aka Perceptrons), which are essentially a mathematical function in the from of `y = Wx + b` where W is the weight and b is the biase. Also, typically the output of a perceptron is passed to an [activation function which normalizes the result](https://en.wikipedia.org/wiki/Activation_function). \n",
    "\n",
    "A CNN is basically a NN with one or more of its hidden layers, typically first, being a [Convolution](https://en.wikipedia.org/wiki/Convolution). This allows the NN to extract features from input such as images.\n",
    "\n",
    "![NN](https://i.imgur.com/xPMkf4i.png)\n",
    "\n",
    "```\n",
    "Baced on 'A deep MNIST classifier using convolutional layers'.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "This tutorial uses [Tensorflow](https://www.tensorflow.org/), which is an open source library released by Google. \n",
    "\n",
    "In order to be able to build, test, and run a NN in Tensorflow, the following imports have to be used.\n",
    "This also imports the MNIST data set (each point in the data set is a handwritten reprentation of the digits 0-9 in 784 pixels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20190223152212-0000\n",
      "KERNEL_ID = e13dd1b4-5fe7-418e-b97d-87f8764709b0\n"
     ]
    }
   ],
   "source": [
    "\"\"\"A deep MNIST classifier using convolutional layers.\n",
    "See extensive documentation at\n",
    "https://www.tensorflow.org/get_started/mnist/pros\n",
    "\"\"\"\n",
    "\n",
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import gzip, sys, os, time\n",
    "import urllib, numpy\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Images source and details\n",
    "SOURCE_URL = 'https://storage.googleapis.com/cvdf-datasets/mnist/'\n",
    "WORK_DIRECTORY = 'data'\n",
    "IMAGE_SIZE = 28\n",
    "NUM_CHANNELS = 1\n",
    "PIXEL_DEPTH = 255\n",
    "NUM_LABELS = 10\n",
    "VALIDATION_SIZE = 5000  # Size of the validation set.\n",
    "SEED = 66478  # Set to None for random seed.\n",
    "\n",
    "# Train/ Eval Params\n",
    "BATCH_SIZE = 64\n",
    "NUM_EPOCHS = 10\n",
    "EVAL_BATCH_SIZE = 64\n",
    "EVAL_FREQUENCY = 100  # Number of steps between evaluations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions used to download the data set and evaluate the results\n",
    "\n",
    "def data_type():\n",
    "    \"\"\"Return the type of the activations, weights, and placeholder variables.\"\"\"\n",
    "    return tf.float32\n",
    "\n",
    "\n",
    "def maybe_download(filename):\n",
    "    \"\"\"Download the data from source, unless it's already here.\"\"\"\n",
    "    if not tf.gfile.Exists(WORK_DIRECTORY):\n",
    "        tf.gfile.MakeDirs(WORK_DIRECTORY)\n",
    "    filepath = os.path.join(WORK_DIRECTORY, filename)\n",
    "    if not tf.gfile.Exists(filepath):\n",
    "        filepath, _ = urllib.request.urlretrieve(SOURCE_URL + filename, filepath)\n",
    "    with tf.gfile.GFile(filepath) as f:\n",
    "        size = f.size()\n",
    "    \n",
    "    print('Successfully downloaded', filename, size, 'bytes.')\n",
    "    return filepath\n",
    "\n",
    "\n",
    "def extract_data(filename, num_images):\n",
    "    \"\"\"Extract the images into a 4D tensor [image index, y, x, channels].\n",
    "    Values are rescaled from [0, 255] down to [-0.5, 0.5].\n",
    "    \"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(16)\n",
    "        buf = bytestream.read(IMAGE_SIZE * IMAGE_SIZE * num_images * NUM_CHANNELS)\n",
    "        data = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.float32)\n",
    "        data = (data - (PIXEL_DEPTH / 2.0)) / PIXEL_DEPTH\n",
    "        data = data.reshape(num_images, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS)\n",
    "    return data\n",
    "\n",
    "\n",
    "def extract_labels(filename, num_images):\n",
    "    \"\"\"Extract the labels into a vector of int64 label IDs.\"\"\"\n",
    "    print('Extracting', filename)\n",
    "    with gzip.open(filename) as bytestream:\n",
    "        bytestream.read(8)\n",
    "        buf = bytestream.read(1 * num_images)\n",
    "        labels = numpy.frombuffer(buf, dtype=numpy.uint8).astype(numpy.int64)\n",
    "    return labels\n",
    "\n",
    "\n",
    "\n",
    "def error_rate(predictions, labels):\n",
    "    \"\"\"Return the error rate based on dense predictions and sparse labels.\"\"\"\n",
    "    return 100.0 - (\n",
    "        100.0 *\n",
    "        numpy.sum(numpy.argmax(predictions, 1) == labels) /\n",
    "        predictions.shape[0])\n",
    "\n",
    "# Small utility function to evaluate a dataset by feeding batches of data to\n",
    "# {eval_data} and pulling the results from {eval_predictions}.\n",
    "# Saves memory and enables this to run on smaller GPUs.\n",
    "def eval_in_batches(data, sess):\n",
    "    \"\"\"Get all predictions for a dataset by running it in small batches.\"\"\"\n",
    "    size = data.shape[0]\n",
    "    if size < EVAL_BATCH_SIZE:\n",
    "        raise ValueError(\"batch size for evals larger than dataset: %d\" % size)\n",
    "    predictions = numpy.ndarray(shape=(size, NUM_LABELS), dtype=numpy.float32)\n",
    "    for begin in range(0, size, EVAL_BATCH_SIZE):\n",
    "        end = begin + EVAL_BATCH_SIZE\n",
    "        if end <= size:\n",
    "            predictions[begin:end, :] = sess.run(eval_prediction, feed_dict={eval_data: data[begin:end, ...]})\n",
    "        else:\n",
    "            batch_predictions = sess.run(eval_prediction, feed_dict={eval_data: data[-EVAL_BATCH_SIZE:, ...]})\n",
    "            predictions[begin:, :] = batch_predictions[begin - size:, :]\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Blocks\n",
    "\n",
    "In order to build a CNN, we need to define the basic building blocks: A convolution function, a max pool function, a weight, and a biase. These 4 functions will be later combined form the perceptrons and different layers in the CNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5x5 filter, depth 32.\n",
    "conv1_weights = tf.Variable(tf.truncated_normal(\n",
    "                        [5, 5, NUM_CHANNELS, 32],  \n",
    "                        stddev=0.1,\n",
    "                        seed=SEED, dtype=data_type()))\n",
    "conv1_biases = tf.Variable(tf.zeros([32], dtype=data_type()))\n",
    "conv2_weights = tf.Variable(tf.truncated_normal(\n",
    "                        [5, 5, 32, 64], stddev=0.1,\n",
    "                        seed=SEED, dtype=data_type()))\n",
    "conv2_biases = tf.Variable(tf.constant(0.1, shape=[64], dtype=data_type()))\n",
    "# fully connected, depth 512.\n",
    "fc1_weights = tf.Variable(  \n",
    "                        tf.truncated_normal([IMAGE_SIZE // 4 * IMAGE_SIZE // 4 * 64, 512],\n",
    "                        stddev=0.1,\n",
    "                        seed=SEED,\n",
    "                        dtype=data_type()))\n",
    "fc1_biases = tf.Variable(tf.constant(0.1, shape=[512], dtype=data_type()))\n",
    "fc2_weights = tf.Variable(tf.truncated_normal([512, NUM_LABELS],\n",
    "                                            stddev=0.1,\n",
    "                                            seed=SEED,\n",
    "                                            dtype=data_type()))\n",
    "fc2_biases = tf.Variable(tf.constant(0.1, shape=[NUM_LABELS], dtype=data_type()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Neural Network\n",
    "\n",
    "This section of the code focuses on defining the CNN and the structure of the CNN. First the input is reshaped into a 28x28x1 tensor (array). \n",
    "\n",
    "In the first layer: the input is convoluted with by a weight and then the biase is added. The result is then normalised using the relu function. \n",
    "\n",
    "In the second layer: max pooling is applied to reduce the dimentionality after applying the convolution.\n",
    "\n",
    "These two layers are repeated twice.\n",
    "\n",
    "Then, then result of the second pooling layer is passed to a connected layer, then is passed through a dropout filter to reduce overfitting, and eventually the recuded-complexity \n",
    "model is passed to an output connected layer.\n",
    "\n",
    "The output layer returns one of 10 classes pertaining to the digits 0-9."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data, train=False):\n",
    "    \"\"\"The Model definition.\"\"\"\n",
    "    # 2D convolution, with 'SAME' padding (i.e. the output feature map has\n",
    "    # the same size as the input). Note that {strides} is a 4D array whose\n",
    "    # shape matches the data layout: [image index, y, x, depth].\n",
    "    conv = tf.nn.conv2d(data,\n",
    "                        conv1_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    # Bias and rectified linear non-linearity.\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv1_biases))\n",
    "    # Max pooling. The kernel size spec {ksize} also follows the layout of\n",
    "    # the data. Here we have a pooling window of 2, and a stride of 2.\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "    conv = tf.nn.conv2d(pool,\n",
    "                        conv2_weights,\n",
    "                        strides=[1, 1, 1, 1],\n",
    "                        padding='SAME')\n",
    "    relu = tf.nn.relu(tf.nn.bias_add(conv, conv2_biases))\n",
    "    pool = tf.nn.max_pool(relu,\n",
    "                        ksize=[1, 2, 2, 1],\n",
    "                        strides=[1, 2, 2, 1],\n",
    "                        padding='SAME')\n",
    "    # Reshape the feature map cuboid into a 2D matrix to feed it to the\n",
    "    # fully connected layers.\n",
    "    pool_shape = pool.get_shape().as_list()\n",
    "    reshape = tf.reshape(pool,[pool_shape[0], pool_shape[1] * pool_shape[2] * pool_shape[3]])\n",
    "    # Fully connected layer. Note that the '+' operation automatically\n",
    "    # broadcasts the biases.\n",
    "    hidden = tf.nn.relu(tf.matmul(reshape, fc1_weights) + fc1_biases)\n",
    "    # Add a 50% dropout during training only. Dropout also scales\n",
    "    # activations such that no rescaling is needed at evaluation time.\n",
    "    if train:\n",
    "        hidden = tf.nn.dropout(hidden, 0.5, seed=SEED)\n",
    "    return tf.matmul(hidden, fc2_weights) + fc2_biases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the CNN\n",
    "\n",
    "Having defied the CNN, we then build it.\n",
    "\n",
    "First: the data set is downloaded.\n",
    "\n",
    "Second: the input and outpu are defined as `x` and `_y` and the CNN is applied to `x`.\n",
    "\n",
    "Then, the cross entropy is defined as the error function to be reduced and the training method is defined as well.\n",
    "\n",
    "Finally, the accuracy is calculated and the Tensorflow graph is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting data/train-images-idx3-ubyte.gz\n",
      "Extracting data/train-labels-idx1-ubyte.gz\n",
      "Extracting data/t10k-images-idx3-ubyte.gz\n",
      "Extracting data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "# Import data\n",
    "train_data_filename = maybe_download('train-images-idx3-ubyte.gz')\n",
    "train_labels_filename = maybe_download('train-labels-idx1-ubyte.gz')\n",
    "test_data_filename = maybe_download('t10k-images-idx3-ubyte.gz')\n",
    "test_labels_filename = maybe_download('t10k-labels-idx1-ubyte.gz')\n",
    "\n",
    "# Extract it into numpy arrays.\n",
    "train_data = extract_data(train_data_filename, 60000)\n",
    "train_labels = extract_labels(train_labels_filename, 60000)\n",
    "test_data = extract_data(test_data_filename, 10000)\n",
    "test_labels = extract_labels(test_labels_filename, 10000)\n",
    "\n",
    "# Generate a validation set.\n",
    "validation_data = train_data[:VALIDATION_SIZE, ...]\n",
    "validation_labels = train_labels[:VALIDATION_SIZE]\n",
    "train_data = train_data[VALIDATION_SIZE:, ...]\n",
    "train_labels = train_labels[VALIDATION_SIZE:]\n",
    "num_epochs = NUM_EPOCHS\n",
    "train_size = train_labels.shape[0]\n",
    "\n",
    "# This is where training samples and labels are fed to the graph.\n",
    "# These placeholder nodes will be fed a batch of training data at each\n",
    "# training step using the {feed_dict} argument to the Run() call below.\n",
    "train_data_node = tf.placeholder( data_type(), shape=(BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))\n",
    "train_labels_node = tf.placeholder(tf.int64, shape=(BATCH_SIZE,))\n",
    "eval_data = tf.placeholder( data_type(), shape=(EVAL_BATCH_SIZE, IMAGE_SIZE, IMAGE_SIZE, NUM_CHANNELS))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the CNN\n",
    "\n",
    "The last step is doing the actual training of the CNN. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Training computation: logits + cross-entropy loss.\n",
    "logits = model(train_data_node, True)\n",
    "loss = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=train_labels_node, logits=logits))\n",
    "\n",
    "# L2 regularization for the fully connected parameters.\n",
    "regularizers = (tf.nn.l2_loss(fc1_weights) + tf.nn.l2_loss(fc1_biases) +\n",
    "              tf.nn.l2_loss(fc2_weights) + tf.nn.l2_loss(fc2_biases))\n",
    "# Add the regularization term to the loss.\n",
    "loss += 5e-4 * regularizers\n",
    "\n",
    "# Optimizer: set up a variable that's incremented once per batch and\n",
    "# controls the learning rate decay.\n",
    "batch = tf.Variable(0, dtype=data_type())\n",
    "# Decay once per epoch, using an exponential schedule starting at 0.01.\n",
    "learning_rate = tf.train.exponential_decay(\n",
    "                  0.01,                # Base learning rate.\n",
    "                  batch * BATCH_SIZE,  # Current index into the dataset.\n",
    "                  train_size,          # Decay step.\n",
    "                  0.95,                # Decay rate.\n",
    "                  staircase=True)\n",
    "# Use simple momentum for the optimization.\n",
    "optimizer = tf.train.MomentumOptimizer(learning_rate,0.9).minimize(loss, global_step=batch)\n",
    "\n",
    "# Predictions for the current training minibatch.\n",
    "train_prediction = tf.nn.softmax(logits)\n",
    "\n",
    "# Predictions for the test and validation, which we'll compute less often.\n",
    "eval_prediction = tf.nn.softmax(model(eval_data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Running the next cell will start the training and test, this will take a long time if you don't have a GPU (around 20 minutes on my machine).\n",
    "\n",
    "Results: \n",
    "```\n",
    "Minibatch loss: 1.607, learning rate: 0.006302\n",
    "Minibatch error: 0.0%\n",
    "Validation error: 0.8%\n",
    "Test error: 0.7%\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized!\n",
      "Step 0 (epoch 0.00), 3.3 ms\n",
      "Minibatch loss: 8.334, learning rate: 0.010000\n",
      "Minibatch error: 85.9%\n",
      "Validation error: 84.6%\n",
      "Step 100 (epoch 0.12), 225.2 ms\n",
      "Minibatch loss: 3.243, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 7.7%\n",
      "Step 200 (epoch 0.23), 221.7 ms\n",
      "Minibatch loss: 3.346, learning rate: 0.010000\n",
      "Minibatch error: 7.8%\n",
      "Validation error: 4.3%\n",
      "Step 300 (epoch 0.35), 222.2 ms\n",
      "Minibatch loss: 3.141, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 3.0%\n",
      "Step 400 (epoch 0.47), 222.8 ms\n",
      "Minibatch loss: 3.194, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.7%\n",
      "Step 500 (epoch 0.58), 221.2 ms\n",
      "Minibatch loss: 3.190, learning rate: 0.010000\n",
      "Minibatch error: 6.2%\n",
      "Validation error: 2.6%\n",
      "Step 600 (epoch 0.70), 222.8 ms\n",
      "Minibatch loss: 3.146, learning rate: 0.010000\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.9%\n",
      "Step 700 (epoch 0.81), 221.2 ms\n",
      "Minibatch loss: 2.968, learning rate: 0.010000\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 2.2%\n",
      "Step 800 (epoch 0.93), 222.1 ms\n",
      "Minibatch loss: 3.025, learning rate: 0.010000\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 2.1%\n",
      "Step 900 (epoch 1.05), 222.0 ms\n",
      "Minibatch loss: 2.907, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.8%\n",
      "Step 1000 (epoch 1.16), 221.8 ms\n",
      "Minibatch loss: 2.855, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.9%\n",
      "Step 1100 (epoch 1.28), 222.9 ms\n",
      "Minibatch loss: 2.821, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.4%\n",
      "Step 1200 (epoch 1.40), 221.1 ms\n",
      "Minibatch loss: 2.934, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.6%\n",
      "Step 1300 (epoch 1.51), 222.9 ms\n",
      "Minibatch loss: 2.800, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.8%\n",
      "Step 1400 (epoch 1.63), 223.0 ms\n",
      "Minibatch loss: 2.839, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.5%\n",
      "Step 1500 (epoch 1.75), 223.0 ms\n",
      "Minibatch loss: 2.862, learning rate: 0.009500\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.3%\n",
      "Step 1600 (epoch 1.86), 223.1 ms\n",
      "Minibatch loss: 2.727, learning rate: 0.009500\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1700 (epoch 1.98), 222.1 ms\n",
      "Minibatch loss: 2.661, learning rate: 0.009500\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.6%\n",
      "Step 1800 (epoch 2.09), 221.2 ms\n",
      "Minibatch loss: 2.658, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.3%\n",
      "Step 1900 (epoch 2.21), 223.7 ms\n",
      "Minibatch loss: 2.640, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2000 (epoch 2.33), 223.3 ms\n",
      "Minibatch loss: 2.633, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.2%\n",
      "Step 2100 (epoch 2.44), 222.7 ms\n",
      "Minibatch loss: 2.584, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.1%\n",
      "Step 2200 (epoch 2.56), 221.2 ms\n",
      "Minibatch loss: 2.565, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 2300 (epoch 2.68), 221.8 ms\n",
      "Minibatch loss: 2.623, learning rate: 0.009025\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 2400 (epoch 2.79), 220.2 ms\n",
      "Minibatch loss: 2.519, learning rate: 0.009025\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2500 (epoch 2.91), 220.8 ms\n",
      "Minibatch loss: 2.470, learning rate: 0.009025\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 2600 (epoch 3.03), 223.0 ms\n",
      "Minibatch loss: 2.470, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.4%\n",
      "Step 2700 (epoch 3.14), 222.0 ms\n",
      "Minibatch loss: 2.497, learning rate: 0.008574\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 1.2%\n",
      "Step 2800 (epoch 3.26), 223.0 ms\n",
      "Minibatch loss: 2.496, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 2900 (epoch 3.37), 222.2 ms\n",
      "Minibatch loss: 2.498, learning rate: 0.008574\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 3000 (epoch 3.49), 223.0 ms\n",
      "Minibatch loss: 2.380, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3100 (epoch 3.61), 221.8 ms\n",
      "Minibatch loss: 2.364, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3200 (epoch 3.72), 221.9 ms\n",
      "Minibatch loss: 2.333, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.2%\n",
      "Step 3300 (epoch 3.84), 221.1 ms\n",
      "Minibatch loss: 2.320, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3400 (epoch 3.96), 220.2 ms\n",
      "Minibatch loss: 2.297, learning rate: 0.008574\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.1%\n",
      "Step 3500 (epoch 4.07), 223.7 ms\n",
      "Minibatch loss: 2.280, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3600 (epoch 4.19), 223.1 ms\n",
      "Minibatch loss: 2.249, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 3700 (epoch 4.31), 222.0 ms\n",
      "Minibatch loss: 2.228, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 3800 (epoch 4.42), 222.0 ms\n",
      "Minibatch loss: 2.215, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 3900 (epoch 4.54), 221.0 ms\n",
      "Minibatch loss: 2.248, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 4000 (epoch 4.65), 222.0 ms\n",
      "Minibatch loss: 2.217, learning rate: 0.008145\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 4100 (epoch 4.77), 221.0 ms\n",
      "Minibatch loss: 2.177, learning rate: 0.008145\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 4200 (epoch 4.89), 221.2 ms\n",
      "Minibatch loss: 2.158, learning rate: 0.008145\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4300 (epoch 5.00), 222.0 ms\n",
      "Minibatch loss: 2.186, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 0.9%\n",
      "Step 4400 (epoch 5.12), 221.8 ms\n",
      "Minibatch loss: 2.134, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4500 (epoch 5.24), 221.2 ms\n",
      "Minibatch loss: 2.174, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.0%\n",
      "Step 4600 (epoch 5.35), 221.0 ms\n",
      "Minibatch loss: 2.085, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4700 (epoch 5.47), 221.7 ms\n",
      "Minibatch loss: 2.066, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 4800 (epoch 5.59), 221.0 ms\n",
      "Minibatch loss: 2.062, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 4900 (epoch 5.70), 223.2 ms\n",
      "Minibatch loss: 2.074, learning rate: 0.007738\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.1%\n",
      "Step 5000 (epoch 5.82), 222.7 ms\n",
      "Minibatch loss: 2.084, learning rate: 0.007738\n",
      "Minibatch error: 4.7%\n",
      "Validation error: 1.0%\n",
      "Step 5100 (epoch 5.93), 222.3 ms\n",
      "Minibatch loss: 2.002, learning rate: 0.007738\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5200 (epoch 6.05), 222.8 ms\n",
      "Minibatch loss: 2.070, learning rate: 0.007351\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 1.0%\n",
      "Step 5300 (epoch 6.17), 222.3 ms\n",
      "Minibatch loss: 1.970, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5400 (epoch 6.28), 222.0 ms\n",
      "Minibatch loss: 1.963, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 1.0%\n",
      "Step 5500 (epoch 6.40), 222.7 ms\n",
      "Minibatch loss: 1.959, learning rate: 0.007351\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 5600 (epoch 6.52), 221.3 ms\n",
      "Minibatch loss: 1.930, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5700 (epoch 6.63), 221.7 ms\n",
      "Minibatch loss: 1.917, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5800 (epoch 6.75), 223.2 ms\n",
      "Minibatch loss: 1.903, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 5900 (epoch 6.87), 221.0 ms\n",
      "Minibatch loss: 1.889, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6000 (epoch 6.98), 223.1 ms\n",
      "Minibatch loss: 1.898, learning rate: 0.007351\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6100 (epoch 7.10), 223.0 ms\n",
      "Minibatch loss: 1.871, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6200 (epoch 7.21), 221.1 ms\n",
      "Minibatch loss: 1.844, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6300 (epoch 7.33), 222.0 ms\n",
      "Minibatch loss: 1.837, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6400 (epoch 7.45), 222.8 ms\n",
      "Minibatch loss: 1.836, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 6500 (epoch 7.56), 221.2 ms\n",
      "Minibatch loss: 1.812, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6600 (epoch 7.68), 221.9 ms\n",
      "Minibatch loss: 1.845, learning rate: 0.006983\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.9%\n",
      "Step 6700 (epoch 7.80), 222.9 ms\n",
      "Minibatch loss: 1.781, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 6800 (epoch 7.91), 223.3 ms\n",
      "Minibatch loss: 1.774, learning rate: 0.006983\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 6900 (epoch 8.03), 221.9 ms\n",
      "Minibatch loss: 1.758, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7000 (epoch 8.15), 222.0 ms\n",
      "Minibatch loss: 1.751, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7100 (epoch 8.26), 222.9 ms\n",
      "Minibatch loss: 1.735, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7200 (epoch 8.38), 223.1 ms\n",
      "Minibatch loss: 1.733, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7300 (epoch 8.49), 221.8 ms\n",
      "Minibatch loss: 1.768, learning rate: 0.006634\n",
      "Minibatch error: 3.1%\n",
      "Validation error: 0.7%\n",
      "Step 7400 (epoch 8.61), 222.2 ms\n",
      "Minibatch loss: 1.700, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.7%\n",
      "Step 7500 (epoch 8.73), 221.8 ms\n",
      "Minibatch loss: 1.692, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 7600 (epoch 8.84), 222.3 ms\n",
      "Minibatch loss: 1.750, learning rate: 0.006634\n",
      "Minibatch error: 1.6%\n",
      "Validation error: 0.8%\n",
      "Step 7700 (epoch 8.96), 221.0 ms\n",
      "Minibatch loss: 1.666, learning rate: 0.006634\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7800 (epoch 9.08), 222.7 ms\n",
      "Minibatch loss: 1.656, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 7900 (epoch 9.19), 221.3 ms\n",
      "Minibatch loss: 1.648, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8000 (epoch 9.31), 222.8 ms\n",
      "Minibatch loss: 1.650, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8100 (epoch 9.43), 222.0 ms\n",
      "Minibatch loss: 1.635, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8200 (epoch 9.54), 224.8 ms\n",
      "Minibatch loss: 1.620, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Step 8300 (epoch 9.66), 221.1 ms\n",
      "Minibatch loss: 1.608, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8400 (epoch 9.77), 222.0 ms\n",
      "Minibatch loss: 1.598, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.8%\n",
      "Step 8500 (epoch 9.89), 223.1 ms\n",
      "Minibatch loss: 1.601, learning rate: 0.006302\n",
      "Minibatch error: 0.0%\n",
      "Validation error: 0.9%\n",
      "Test error: 0.7%\n"
     ]
    }
   ],
   "source": [
    "# Create a local session to run the training.\n",
    "start_time = time.time()\n",
    "with tf.Session() as sess:\n",
    "    # Run all the initializers to prepare the trainable parameters.\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized!')\n",
    "    # Loop through training steps.\n",
    "    for step in range(int(num_epochs * train_size) // BATCH_SIZE):\n",
    "        # Compute the offset of the current minibatch in the data.\n",
    "        # Note that we could use better randomization across epochs.\n",
    "        offset = (step * BATCH_SIZE) % (train_size - BATCH_SIZE)\n",
    "        batch_data = train_data[offset:(offset + BATCH_SIZE), ...]\n",
    "        batch_labels = train_labels[offset:(offset + BATCH_SIZE)]\n",
    "        \n",
    "        # This dictionary maps the batch data (as a numpy array) to the\n",
    "        # node in the graph it should be fed to.\n",
    "        feed_dict = {train_data_node: batch_data, train_labels_node: batch_labels}\n",
    "        \n",
    "        # Run the optimizer to update weights.\n",
    "        sess.run(optimizer, feed_dict=feed_dict)\n",
    "        # print some extra information once reach the evaluation frequency\n",
    "        \n",
    "        if step % EVAL_FREQUENCY == 0:\n",
    "            # fetch some extra nodes' data\n",
    "            l, lr, predictions = sess.run([loss, learning_rate, train_prediction], feed_dict=feed_dict)\n",
    "            elapsed_time = time.time() - start_time\n",
    "            start_time = time.time()\n",
    "            print('Step %d (epoch %.2f), %.1f ms' %\n",
    "                    (step, float(step) * BATCH_SIZE / train_size,\n",
    "                    1000 * elapsed_time / EVAL_FREQUENCY))\n",
    "            print('Minibatch loss: %.3f, learning rate: %.6f' % (l, lr))\n",
    "            print('Minibatch error: %.1f%%' % error_rate(predictions, batch_labels))\n",
    "            print('Validation error: %.1f%%' % \n",
    "                    error_rate(eval_in_batches(validation_data, sess), validation_labels))\n",
    "            sys.stdout.flush()\n",
    "    \n",
    "    # Finally print the result!\n",
    "    test_error = error_rate(eval_in_batches(test_data, sess), test_labels)\n",
    "    print('Test error: %.1f%%' % test_error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Raw Cell Format",
  "kernelspec": {
   "display_name": "Python 3.5 with Spark",
   "language": "python3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
